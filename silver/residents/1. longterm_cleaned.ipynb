{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f471fd1-b014-41ea-af19-6e9309003dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_udf_utils_residents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b85554b-787b-420c-a5bd-956a09d2d164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode_outer, current_timestamp, sum\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b40e8ba-26a6-4f27-b091-7f7945a5f400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_data(bronze_df):\n",
    "    # since the data is currently JSON string, \n",
    "    # I need to convert it back to JSON object using from_json\n",
    "    resident_schema = StructType([\n",
    "        StructField(\"국적지역\", StringType(), False),\n",
    "        StructField(\"년\", IntegerType(), False),\n",
    "        StructField(\"등록외국인 수\", IntegerType(), False)  \n",
    "    ])\n",
    "    \n",
    "    # using from_json, the column `data` (JSON string) is converted into ArrayType(StructType)\n",
    "    # Each element is now a struct mapping the original JSON object's key-value pairs.\n",
    "    resident_df = bronze_df.withColumn(\"data_parsed\", from_json(col(\"data\"), ArrayType(resident_schema)))\n",
    "\n",
    "    # flatten the array\n",
    "    exploded_df = resident_df.select(explode_outer(col(\"data_parsed\")))\n",
    "\n",
    "    # convert each item in struct as a column\n",
    "    processed_df = exploded_df.select(\n",
    "        col(\"col.국적지역\").alias(\"Nationality\"),\n",
    "        col(\"col.년\").alias(\"Year\"),\n",
    "        col(\"col.`등록외국인 수`\").alias(\"Amount\")\n",
    "    )\n",
    "\n",
    "    return processed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6828e5da-81d6-4698-899f-c4793018a418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get data from bronze\n",
    "spark = SparkSession.builder.appName(\"silver_resident_longterm\").getOrCreate()\n",
    "bronze_df = spark.table(\"workspace.growth_poc.bronze_residents_longterm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e43151-245d-42d2-b0b6-fefc80203292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get data from bronze\n",
    "spark = SparkSession.builder.appName(\"silver_resident_longterm\").getOrCreate()\n",
    "bronze_df = spark.table(\"workspace.growth_poc.bronze_residents_longterm\")\n",
    "\n",
    "# 2. Flatten data\n",
    "processed_df = process_data(bronze_df)\n",
    "\n",
    "# 3. Clean up nationality \n",
    "cleaned_df = clean_nationality(processed_df) \n",
    "cleaned_df = cleaned_df.groupBy(\"Nationality\", \"Year\").agg(\n",
    "        sum(\"Amount\").alias(\"Amount\")\n",
    ")\n",
    "\n",
    "# 4. Apply mapping\n",
    "mapped_df = map_nationality(cleaned_df)\n",
    "final_df = mapped_df.withColumn(\"TimeStamp\", current_timestamp())\n",
    "\n",
    "final_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"mergeSchema\", True)\\\n",
    "        .saveAsTable(\"workspace.growth_poc.silver_residents_longterm\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6964065237681828,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. longterm_cleaned",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
