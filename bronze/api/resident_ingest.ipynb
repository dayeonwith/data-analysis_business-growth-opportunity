{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b12d9d2-f029-4a7e-8901-23e86e6f74f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install library\n",
    "# %pip install requests\n",
    "# %pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce4a3a3-c9c9-4acf-bbca-d2f35a037c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# read data from api\n",
    "def read_data(base_url, endpoint, api_key):\n",
    "    # 1. api information\n",
    "    api_endpoint = endpoint\n",
    "    url = f\"{base_url}{api_endpoint}\"\n",
    "\n",
    "    # set initial variables\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    perPage = 500\n",
    "    api_key = api_key    \n",
    "\n",
    "    # 2. get data\n",
    "    while True:\n",
    "        params = {\n",
    "            \"page\" : page,\n",
    "            \"perPage\" : perPage,\n",
    "            \"serviceKey\" : api_key\n",
    "        }\n",
    "        # send api request\n",
    "        response = requests.get(url, params)\n",
    "        # check request status\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # using .get() in case there is no data returned. In that case, new_data will be []\n",
    "            new_data = data.get(\"data\", []) \n",
    "            # new_data is empty, escape the loop\n",
    "            if len(new_data) == 0:\n",
    "                break\n",
    "            # add the data to the all_data\n",
    "            all_data.append(new_data)\n",
    "            # move to the next page\n",
    "            page += 1\n",
    "        # error on request -> escape the look\n",
    "        else:\n",
    "            print(f\"error code: {response.status_code}\")\n",
    "            print(f\"error message: {response.text}\")\n",
    "            # side note: if this was a ELT pipeline, unless the error is 4xx, \n",
    "            # I would have implemented retry mechanism with incremental gaps \n",
    "            # between the tries before failing the pipeline, maybe three times.\n",
    "            break\n",
    "    return all_data\n",
    "\n",
    "# load environment variables from .env\n",
    "load_dotenv() \n",
    "## call API_KEY saved in .env\n",
    "api_key = os.getenv(\"API_KEY\") \n",
    "base_url = \"https://api.odcloud.kr/api\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b4e2db-0714-49a3-9a2c-ee566c84ebfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get a list of endpoint from a textfile\n",
    "with open(\"resident_endpoints.txt\", \"r\") as f:\n",
    "    endpoints_to_read = [line.strip() for line in f]\n",
    "\n",
    "total_results = {}\n",
    "i = 1\n",
    "# loop through each endpoint \n",
    "for ep in endpoints_to_read:\n",
    "    print(f\"processing.. {i}/{len(endpoints_to_read)}\")\n",
    "    result_data = read_data(base_url, ep, api_key)\n",
    "    total_results[ep] = result_data\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "print(\"--- All processes finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c963605d-9b38-4593-8341-dfedd63cb80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare data for dataframe\n",
    "# I will pair endpoint and values\n",
    "# currently, total_results is a dictionary with:\n",
    "# key : endpoint\n",
    "# value : nested list of json object lists from each page\n",
    "import json\n",
    "\n",
    "bronze_data = []\n",
    "# Unnests pages and records, pairing each record with its source endpoint.\n",
    "for endpoint, pages in total_results.items():\n",
    "    # flatten nested records from each page in\n",
    "    records = [item for page in pages for item in page]\n",
    "    # same as: \n",
    "    #   for page in pages:\n",
    "    #       for item in page:\n",
    "    #           records.append(item)\n",
    "\n",
    "    bronze_data.append((endpoint, json.dumps(records, ensure_ascii=False))) \n",
    "    # dumps: dump+s(string)\n",
    "    # ensure_ascii= False: keep Korean words as is\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ff2ab4-02b6-4e48-b430-add46539693c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"resident_ingest\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f6ab7b-3172-4322-8113-c171da829cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "apidata_schema = StructType([\n",
    "    StructField(\"endpoint\", StringType(), False),\n",
    "    StructField(\"value\", StringType(), False)\n",
    "])\n",
    "df = spark.createDataFrame(data = bronze_data, schema= apidata_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4e6ad7-280e-423c-a80a-657a1455ecfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .saveAsTable(\"workspace.growth_poc.bronze_residents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3248a8dd-9677-4b7e-885f-42996146fad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b>Flattening the json object here vs in silver layer</b> <br>\n",
    "I prefer doing it in silver layer because: <br>\n",
    "\n",
    "<ol>\n",
    "<li>Preserves the \"Single Source of Truth\"</li> \n",
    "The Bronze layer acts as a historical archive of the source data, exactly as it was received. This is crucial for debugging and tracking history. If I flatten the data here and later realize the business needs a field I discarded, I can't get it back without re-ingesting from the API. Also, I would have lost the historical data of it because I have been excluding it from the ingestion. With the raw JSON, I can simply adjust Silver layer logic and replay from Bronze.\n",
    "\n",
    "<li>Resilience to Schema Changes</li> \n",
    "APIs often change. If a new nested field is added, Bronze ingestion pipeline won't break because it's just saving a string. I can then decide how to handle the new field in the Silver layer without any ingestion downtime.\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "resident_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
